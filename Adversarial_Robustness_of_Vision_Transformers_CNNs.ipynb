{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sosier/Adversarial_Robustness_of_Vision_Transformers_CNNs/blob/main/Adversarial_Robustness_of_Vision_Transformers_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqoSq4JR0z43"
      },
      "source": [
        "# Analysis on Adversarial Robustness of Vision Transformers & CNNs\n",
        "## How Training Decisions Impact Adversarial Robustness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uCuAuaHcNim"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"./data/\"\n",
        "GOOGLE_DRIVE_PATH = \"/content/drive/\"\n",
        "# Your path may differ edit as necessary:\n",
        "TRAINING_RUNS_PATH = GOOGLE_DRIVE_PATH + \"MyDrive/NNDL/Final_Project/Training_Runs/\"\n",
        "# EDIT ME:\n",
        "USER = \"Sean\"  # For collaborating with others on running the experiments\n",
        "RANDOM_SEED = 8888  # Current random seed to test\n",
        "# All random seeds for which experiments were run. To completely replicate\n",
        "# results you will need to run the experiments for each of these:\n",
        "COMPLETED_RANDOM_SEEDS = [8888, 8889, 8891]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBQmjhONAVgF"
      },
      "source": [
        "## Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WJkrGQyGiZB"
      },
      "source": [
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqP6Wltrquf3"
      },
      "outputs": [],
      "source": [
        "# Install if necessary:\n",
        "!pip install einops  # For complicated tensor reshaping\n",
        "!pip install torchattacks  # For generating adversarial attacks\n",
        "!pip3 install git+https://github.com/ponnhide/patchworklib.git  # For plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqW3k8VG0xjq"
      },
      "outputs": [],
      "source": [
        "# Torch related:\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import torchattacks\n",
        "import torchsummary\n",
        "# Experiment management / misc:\n",
        "from google.colab import data_table, drive\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from time import time\n",
        "import math\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "# Plotting related:\n",
        "from IPython.display import display_png\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "import patchworklib as pw\n",
        "from plotnine import (\n",
        "    ggplot, aes, geom_line, theme_minimal, ggtitle, xlab, ylab, theme,\n",
        "    element_text, scale_x_continuous, scale_y_continuous, geom_vline,\n",
        "    geom_text, annotate, scale_color_manual, scale_color_identity,\n",
        "    element_blank, theme_void, guides, guide_legend, geom_bar, scale_fill_manual\n",
        ")\n",
        "import seaborn as sns\n",
        "\n",
        "# Display pandas DataFrames as Google Colab Data Tables:\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "# Mount Google Drive on Colab:\n",
        "drive.mount(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WitDPzOzAfSx"
      },
      "source": [
        "### Download and Preview Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2hkpdJG0zbT"
      },
      "outputs": [],
      "source": [
        "# Will only download once\n",
        "# Without the tranfrom returns PIL Image\n",
        "train_data = MNIST(DATA_PATH, train=True, download=True, transform=ToTensor())\n",
        "test_data = MNIST(DATA_PATH, train=False, download=True, transform=ToTensor())\n",
        "\n",
        "print(f\"{len(train_data)=}\")\n",
        "print(f\"{len(test_data)=}\")\n",
        "train_data[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM0kNgjZORH7"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 4, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
        "    img, label = train_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(label)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8tHEc7dAlzx"
      },
      "source": [
        "### Model Specification, Training, Testing Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline Model Params"
      ],
      "metadata": {
        "id": "IAwnT9QjWyTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VISION_TRANSFORMER_BASELINE = dict(\n",
        "    train_data=train_data,\n",
        "    test_data=test_data,\n",
        "    random_seed=RANDOM_SEED,\n",
        "    # -------------\n",
        "    # Model Params:\n",
        "    # -------------\n",
        "    model_class=\"VisionTransformer\",\n",
        "    # Architecture Size Params:\n",
        "    patch_size=7,\n",
        "    model_dim=16,\n",
        "    head_dim=8,\n",
        "    num_heads=4,\n",
        "    mlp_dim=32,\n",
        "    num_layers=2,\n",
        "    # Other Architecture Params:\n",
        "    position_embedding=\"learned\",  # Standard per original paper\n",
        "    QKV_bias=True,\n",
        "    ## Scaled dot product attention scaling factor. When None, defaults to:\n",
        "    ## 1/sqrt(head_dim)\n",
        "    scaling_factor=None,\n",
        "    activation_function=\"GELU\",  # MLP activation\n",
        "    # Regularization Params:\n",
        "    layer_norm_patches=False,\n",
        "    layer_norm=\"before\",\n",
        "    dropout=0.0,  # Default to no dropout\n",
        "    # ----------------\n",
        "    # Training Params:\n",
        "    # ----------------\n",
        "    optimizer=\"Adam\",\n",
        "    epochs_to_run=5,\n",
        "    batch_size=128,\n",
        "    learning_rate=0.001,\n",
        "    # -----------------------------\n",
        "    # Testing / Adversarial Params:\n",
        "    # -----------------------------\n",
        "    adv_algorithm=\"PGD\",\n",
        "    eps=8/255,\n",
        "    alpha=2/255,\n",
        "    steps=4\n",
        ")\n",
        "\n",
        "CNN_BASELINE = dict(\n",
        "    train_data=train_data,\n",
        "    test_data=test_data,\n",
        "    random_seed=RANDOM_SEED,\n",
        "    # -------------\n",
        "    # Model Params:\n",
        "    # -------------\n",
        "    model_class=\"CNN\",\n",
        "    conv_layer_1=16,\n",
        "    conv_layer_2=32,\n",
        "    conv_layer_3=0,\n",
        "    conv_kernel=3,\n",
        "    conv_stride=1,\n",
        "    conv_padding=1,\n",
        "    max_pool_kernel=2,\n",
        "    activation_function=\"ReLU\",\n",
        "    fully_connected_layer_1=0,\n",
        "    # Regularization Params:\n",
        "    conv_normalization=\"none\",\n",
        "    dropout=0.0,  # Default to no dropout\n",
        "    # ----------------\n",
        "    # Training Params:\n",
        "    # ----------------\n",
        "    optimizer=\"Adam\",\n",
        "    epochs_to_run=5,\n",
        "    batch_size=128,\n",
        "    learning_rate=0.001,\n",
        "    # -----------------------------\n",
        "    # Testing / Adversarial Params:\n",
        "    # -----------------------------\n",
        "    adv_algorithm=\"PGD\",\n",
        "    eps=25/255,\n",
        "    alpha=5/255,\n",
        "    steps=10\n",
        ")"
      ],
      "metadata": {
        "id": "ggZ4gDh9WxSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vision Transformer"
      ],
      "metadata": {
        "id": "yIXKnKcqcvPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageToPatches(nn.Module):\n",
        "    \"\"\"\n",
        "    Breaks images (`X`) into square patches of `patch_size` x `patch_size`\n",
        "    pixels\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size=7):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # b, c, h, w = batch_size, channels, height, width\n",
        "        self.to_patches = Rearrange(\n",
        "            \"b c (h ph) (w pw) -> b (h w) (ph pw c)\",\n",
        "            ph=self.patch_size,\n",
        "            pw=self.patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        b, c, h, w = X.shape  # batch_size, channels, height, width\n",
        "        assert (\n",
        "            h % self.patch_size == 0 and w % self.patch_size == 0\n",
        "        ), \"image height and width should be divisible by patch_size\"\n",
        "\n",
        "        return self.to_patches(X)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified version of standard Transformer positional embedding from PyTorch\n",
        "    documentation:\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.shape[1]]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_out_dim,  # a.k.a. model_dim\n",
        "        head_dim=64,\n",
        "        num_heads=8,\n",
        "        layer_norm=\"before\",\n",
        "        QKV_bias=True,\n",
        "        # Scaled dot product attention scaling factor. Defaults to:\n",
        "        # 1/sqrt(head_dim)\n",
        "        scaling_factor=None,\n",
        "        dropout=0.0,  # Default to no dropout\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert(layer_norm in (\"before\", \"after\", \"both\", \"none\"))\n",
        "\n",
        "        self.layer_norm = layer_norm\n",
        "        if self.layer_norm in (\"before\", \"both\"):\n",
        "            self.layer_norm_before = nn.LayerNorm(in_out_dim)\n",
        "\n",
        "        # For efficiency calculate Q, K, and V for all heads in one operation:\n",
        "        self.head_dim = head_dim\n",
        "        self.num_heads = num_heads\n",
        "        multihead_dim = self.head_dim * self.num_heads\n",
        "        self.calc_QKV = nn.Linear(in_out_dim, multihead_dim * 3, bias=QKV_bias)\n",
        "\n",
        "        self.scaling_factor = scaling_factor\n",
        "        if not self.scaling_factor:\n",
        "            self.scaling_factor = self.head_dim**-0.5\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        needs_output_layer = self.num_heads > 1 or self.head_dim != in_out_dim\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(multihead_dim, in_out_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if needs_output_layer else nn.Identity()\n",
        "\n",
        "        if self.layer_norm in (\"after\", \"both\"):\n",
        "            self.layer_norm_after = nn.LayerNorm(in_out_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        batch_size, sequence_length, in_out_dim = X.shape  # b, s, d\n",
        "\n",
        "        if self.layer_norm in (\"before\", \"both\"):\n",
        "            X = self.layer_norm_before(X)\n",
        "\n",
        "        # Calculate and reshape Q, K, V:\n",
        "        Q, K, V = (\n",
        "            rearrange(\n",
        "                t,\n",
        "                \"b s (num_heads head_dim) -> b num_heads s head_dim\",\n",
        "                num_heads=self.num_heads,\n",
        "                head_dim=self.head_dim\n",
        "            )\n",
        "            for t in self.calc_QKV(X).chunk(3, dim=-1)\n",
        "        )\n",
        "\n",
        "        unnormalized_attention = (Q @ K.transpose(-1, -2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        Z = attention_weights @ V\n",
        "        # Reshape Z to undo the Q, K, V reshaping above:\n",
        "        Z = rearrange(\n",
        "                Z,\n",
        "                \"b num_heads s head_dim -> b s (num_heads head_dim)\",\n",
        "                num_heads=self.num_heads,\n",
        "                head_dim=self.head_dim\n",
        "            )\n",
        "\n",
        "        Z = self.output(Z)\n",
        "\n",
        "        if self.layer_norm in (\"after\", \"both\"):\n",
        "            Z = self.layer_norm_after(Z)\n",
        "\n",
        "        return Z\n",
        "\n",
        "class TransformerMLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_out_dim,  # a.k.a. model_dim\n",
        "        mlp_dim,\n",
        "        layer_norm=\"before\",\n",
        "        activation_function=\"GELU\",  # GELU is standard per original paper\n",
        "        dropout=0.0,  # Default to no dropout\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert(layer_norm in (\"before\", \"after\", \"both\", \"none\"))\n",
        "\n",
        "        if type(activation_function) == str:\n",
        "            assert(activation_function in nn.__dict__), (\n",
        "                \"activation_function must be one of the activation functions \" +\n",
        "                \"available in the torch.nn module, e.g. 'ReLU'\"\n",
        "            )\n",
        "            activation_function = nn.__dict__[activation_function]\n",
        "\n",
        "        self.layer_norm = layer_norm\n",
        "        if self.layer_norm in (\"before\", \"both\"):\n",
        "            self.layer_norm_before = nn.LayerNorm(in_out_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_out_dim, mlp_dim),\n",
        "            activation_function(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, in_out_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        if self.layer_norm in (\"after\", \"both\"):\n",
        "            self.layer_norm_after = nn.LayerNorm(in_out_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.layer_norm in (\"before\", \"both\"):\n",
        "            X = self.layer_norm_before(X)\n",
        "\n",
        "        X = self.mlp(X)\n",
        "\n",
        "        if self.layer_norm in (\"after\", \"both\"):\n",
        "            X = self.layer_norm_after(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, in_out_dim, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multihead_attention = MultiHeadAttention(in_out_dim, **kwargs)\n",
        "        self.mlp = TransformerMLP(in_out_dim, **kwargs)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X + self.multihead_attention(X)\n",
        "        X = X + self.mlp(X)\n",
        "        return X\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        # Input Details (fixed for MNIST):\n",
        "        image_channels=1,\n",
        "        num_classes=10,\n",
        "        max_patches=28*28,  # For positional embedding\n",
        "        # Architecture Size Params:\n",
        "        patch_size=7,\n",
        "        model_dim=16,\n",
        "        head_dim=8,\n",
        "        num_heads=4,\n",
        "        mlp_dim=32,\n",
        "        num_layers=2,\n",
        "        # Other Architecture Params:\n",
        "        position_embedding=\"learned\",  # Standard per original paper\n",
        "        QKV_bias=True,\n",
        "        ## Scaled dot product attention scaling factor. Defaults to:\n",
        "        ## 1/sqrt(head_dim)\n",
        "        scaling_factor=None,\n",
        "        activation_function=\"GELU\",  # MLP activation\n",
        "        # Regularization Params:\n",
        "        layer_norm_patches=False,\n",
        "        layer_norm=\"before\",\n",
        "        dropout=0.0,  # Default to no dropout\n",
        "        # Training / Evaluation Params:\n",
        "        loss_function=\"cross_entropy\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert(position_embedding in (\"learned\", \"standard_transformer\", \"none\"))\n",
        "\n",
        "        patch_dim = image_channels * patch_size**2\n",
        "        self.image_to_patch_embeddings = nn.Sequential(\n",
        "            ImageToPatches(patch_size),\n",
        "            nn.LayerNorm(patch_dim) if layer_norm_patches else nn.Identity(),\n",
        "            nn.Linear(patch_dim, model_dim),\n",
        "            nn.LayerNorm(model_dim) if layer_norm != \"none\" else nn.Identity()\n",
        "        )\n",
        "\n",
        "        # Token from which we make the final prediction so that we don't bias\n",
        "        # our result to any particular image patch:\n",
        "        self.class_token = nn.Parameter(torch.randn(1, 1, model_dim))\n",
        "\n",
        "        self.position_embedding = position_embedding\n",
        "        if self.position_embedding == \"learned\":\n",
        "            self.add_position_embedding = nn.Parameter(\n",
        "                torch.randn(1, max_patches + 1, model_dim)\n",
        "            )\n",
        "        elif self.position_embedding == \"standard_transformer\":\n",
        "            self.add_position_embedding = PositionalEncoding(\n",
        "                model_dim, dropout, max_len=(max_patches + 1)\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                in_out_dim=model_dim,\n",
        "                head_dim=head_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_dim=mlp_dim,\n",
        "                QKV_bias=QKV_bias,\n",
        "                scaling_factor=scaling_factor,\n",
        "                activation_function=activation_function,\n",
        "                layer_norm=layer_norm,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.layer_norm = layer_norm\n",
        "        if self.layer_norm != \"none\":\n",
        "            self.final_layer_norm = nn.LayerNorm(model_dim)\n",
        "\n",
        "        self.final_output = nn.Linear(model_dim, num_classes)\n",
        "\n",
        "        if type(loss_function) == str:\n",
        "            assert(loss_function in F.__dict__), (\n",
        "                \"loss_function must be one of the loss functions \" +\n",
        "                \"available in the torch.nn.functional module, e.g. 'cross_entropy'\"\n",
        "            )\n",
        "            loss_function = F.__dict__[loss_function]\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def forward(self, X, targets=None):\n",
        "        # Embded images:\n",
        "        X = self.image_to_patch_embeddings(X)\n",
        "        b, s, m = X.shape  # batch_size, sequence_length, model_dim\n",
        "\n",
        "        # Attach a class token to each sequence of image patches:\n",
        "        class_tokens = repeat(self.class_token, \"1 1 m -> b 1 m\", b=b)\n",
        "        X = torch.cat((class_tokens, X), dim=1)\n",
        "\n",
        "        if self.position_embedding == \"learned\":\n",
        "            X = X + self.add_position_embedding[:, :(s + 1)]\n",
        "            X = self.dropout(X)\n",
        "        elif self.position_embedding == \"standard_transformer\":\n",
        "            X = self.add_position_embedding(X)\n",
        "        else:\n",
        "            X = self.dropout(X)\n",
        "\n",
        "        # Run main body of the Transformer:\n",
        "        for layer in self.layers:\n",
        "            X = layer(X)\n",
        "\n",
        "        # Extract class token:\n",
        "        X = X[:, 0]\n",
        "\n",
        "        # Output final logits:\n",
        "        if self.layer_norm != \"none\":\n",
        "            X = self.final_layer_norm(X)\n",
        "\n",
        "        # Final layer's output is the logits:\n",
        "        logits = self.final_output(X)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = self.loss_function(logits, targets)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits\n"
      ],
      "metadata": {
        "id": "GXxdUZyPLZfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN"
      ],
      "metadata": {
        "id": "phTJfbsdPLKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For helping manage layer size inputs to CNN:\n",
        "def layers_dict_to_size_list(layers_dict):\n",
        "    layer_sizes = []\n",
        "    for layer in sorted(layers_dict.items()):\n",
        "        if layer[1]:  # if layer size > 0\n",
        "            layer_sizes.append(layer[1])\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return layer_sizes\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        # Input Details (fixed for MNIST):\n",
        "        in_channels=1,\n",
        "        in_height=28,\n",
        "        in_width=28,\n",
        "        num_classes=10,\n",
        "        loss_function=\"cross_entropy\",\n",
        "        # Architecutre Params:\n",
        "        conv_layers=[16, 32],\n",
        "        conv_kernel=3,\n",
        "        conv_stride=1,\n",
        "        conv_padding=1,\n",
        "        max_pool_kernel=2,\n",
        "        activation_function=\"ReLU\",\n",
        "        fully_connected_layers=[],\n",
        "        # Regularization Params:\n",
        "        conv_normalization=\"none\",  # Set to False so default is no regularization\n",
        "        dropout=0.0,  # Set to 0 so default is no regularization\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # ------------------------\n",
        "        # Validate / Clean Inputs:\n",
        "        # ------------------------\n",
        "        assert(conv_normalization in (\"batch\", \"layer\", \"none\"))\n",
        "\n",
        "        # Support alternative way of defining of define conv_layers and\n",
        "        # full_connected_layers that is more convenient for experiments:\n",
        "        conv_layer_size_params = {\n",
        "            k: v for k, v in kwargs.items()\n",
        "            if k.startswith(\"conv_layer_\")\n",
        "        }\n",
        "        if len(conv_layer_size_params) > 0:\n",
        "            conv_layers = layers_dict_to_size_list(conv_layer_size_params)\n",
        "        assert(len(conv_layers) >= 1)\n",
        "\n",
        "        fully_connected_layer_size_params = {\n",
        "            k: v for k, v in kwargs.items()\n",
        "            if k.startswith(\"fully_connected_layer_\")\n",
        "        }\n",
        "        if len(fully_connected_layer_size_params) > 0:\n",
        "            fully_connected_layers =\\\n",
        "                layers_dict_to_size_list(fully_connected_layer_size_params)\n",
        "\n",
        "        # If necessary, fetch actual activation function and loss functoon:\n",
        "        if type(activation_function) == str:\n",
        "            assert(activation_function in nn.__dict__), (\n",
        "                \"activation_function must be one of the activation functions \" +\n",
        "                \"available in the torch.nn module, e.g. 'ReLU'\"\n",
        "            )\n",
        "            activation_function = nn.__dict__[activation_function]\n",
        "\n",
        "        if type(loss_function) == str:\n",
        "            assert(loss_function in F.__dict__), (\n",
        "                \"loss_function must be one of the loss functions \" +\n",
        "                \"available in the torch.nn.functional module, e.g. 'cross_entropy'\"\n",
        "            )\n",
        "            loss_function = F.__dict__[loss_function]\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "        # ------------------------------\n",
        "        # Actually Initialize the Model:\n",
        "        # ------------------------------\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Convolutional Layers:\n",
        "        for i, layer_out in enumerate(conv_layers):\n",
        "            layer_in = in_channels if i == 0 else conv_layers[i - 1]\n",
        "            max_pool_stride = max_pool_kernel\n",
        "\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=layer_in,\n",
        "                    out_channels=layer_out,\n",
        "                    kernel_size=conv_kernel,\n",
        "                    stride=conv_stride,\n",
        "                    padding=conv_padding\n",
        "                ),\n",
        "                activation_function(),\n",
        "                nn.MaxPool2d(\n",
        "                    kernel_size=max_pool_kernel,\n",
        "                    stride=max_pool_stride\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Update height / width:\n",
        "            in_height, in_width = self.__update_height_width(\n",
        "                in_height,\n",
        "                in_width,\n",
        "                conv_kernel,\n",
        "                conv_stride,\n",
        "                conv_padding,\n",
        "                max_pool_kernel,\n",
        "                max_pool_stride\n",
        "            )\n",
        "\n",
        "            # Normalization (optional):\n",
        "            if conv_normalization == \"batch\":\n",
        "                layer.add_module(f\"BatchNorm{i + 1}\", nn.BatchNorm2d(layer_out))\n",
        "            elif conv_normalization == \"layer\":\n",
        "                layer.add_module(\n",
        "                    f\"LayerNorm{i + 1}\",\n",
        "                    nn.LayerNorm((layer_out, in_height, in_width)))\n",
        "\n",
        "            # Dropout (optional):\n",
        "            if dropout > 0:\n",
        "                layer.add_module(f\"Dropout{i + 1}\", nn.Dropout(dropout))\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        # Flatten Output:\n",
        "        self.layers.append(nn.Flatten())\n",
        "\n",
        "        # Fully Connected Layers:\n",
        "        for i, layer_out in enumerate(fully_connected_layers):\n",
        "            if i > 0:\n",
        "                layer_in = fully_connected_layers[i - 1]\n",
        "            else:\n",
        "                layer_in = conv_layers[-1] * in_height * in_width\n",
        "\n",
        "            layer = nn.Sequential(\n",
        "                nn.Linear(layer_in, layer_out),\n",
        "                activation_function()\n",
        "            )\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        # Final Fully Connected Output Layer:\n",
        "        if len(fully_connected_layers):\n",
        "            layer_in = fully_connected_layers[-1]\n",
        "        else:\n",
        "            layer_in = conv_layers[-1] * in_height * in_width\n",
        "\n",
        "        self.layers.append(nn.Linear(layer_in, num_classes))\n",
        "\n",
        "        # Dropout (optional):\n",
        "        if dropout > 0:\n",
        "            self.layers.append(nn.Dropout(dropout))\n",
        "\n",
        "    def __calc_new_height_or_width(self, old, kernel, stride, padding=0, dilation=1):\n",
        "        \"\"\"\n",
        "        Calculate the height or width of an image after applying a convolution or\n",
        "        max pooling operation\n",
        "\n",
        "        Based on formulas here:\n",
        "        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d:~:text=output.%20Default%3A%20True-,Shape%3A,-Input%3A\n",
        "        \"\"\"\n",
        "        return (old + 2 * padding - dilation * (kernel - 1) - 1) // stride + 1\n",
        "\n",
        "    def __update_height_width(self,\n",
        "            height,\n",
        "            width,\n",
        "            conv_kernel,\n",
        "            conv_stride,\n",
        "            conv_padding,\n",
        "            max_pool_kernel,\n",
        "            max_pool_stride\n",
        "        ):\n",
        "        # Update height / width:\n",
        "        # After convolution\n",
        "        height = self.__calc_new_height_or_width(\n",
        "            height, conv_kernel, conv_stride, conv_padding\n",
        "        )\n",
        "        width = self.__calc_new_height_or_width(\n",
        "            width, conv_kernel, conv_stride, conv_padding\n",
        "        )\n",
        "        # After max pool\n",
        "        height = self.__calc_new_height_or_width(\n",
        "            height, max_pool_kernel, max_pool_stride\n",
        "        )\n",
        "        width = self.__calc_new_height_or_width(\n",
        "            width, max_pool_kernel, max_pool_stride\n",
        "        )\n",
        "\n",
        "        return height, width\n",
        "\n",
        "    def forward(self, X, targets=None):\n",
        "        for layer in self.layers:\n",
        "            X = layer(X)\n",
        "\n",
        "        # Final layer's output is the logits:\n",
        "        logits = X\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = self.loss_function(logits, targets)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits\n"
      ],
      "metadata": {
        "id": "0P70qEsXPI8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training / Testing Code"
      ],
      "metadata": {
        "id": "j8TbE4pPc2O9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tFsJf5DP57z"
      },
      "outputs": [],
      "source": [
        "def _next_X_y(data_loader):\n",
        "    global DEVICE\n",
        "    X, y = next(iter(data_loader))\n",
        "    X = X.to(DEVICE)\n",
        "    y = y.to(DEVICE)\n",
        "    return X, y\n",
        "\n",
        "def _train_on_batch(model, X, y, optimizer):\n",
        "    # Put model into training mode:\n",
        "    model.train()\n",
        "\n",
        "    # Do forward pass and evaluate loss\n",
        "    _, loss = model(X, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def _calculate_accuracy_from_logits(logits, target_y):\n",
        "    return (F.softmax(logits, dim=1).argmax(dim=1) == target_y).float().mean()\n",
        "\n",
        "def _calculate_reporting_metrics(model, test_data_loader):\n",
        "    model.eval()\n",
        "    X, y = _next_X_y(test_data_loader)\n",
        "    logits, loss = model(X, y)\n",
        "    accuracy = _calculate_accuracy_from_logits(logits, y)\n",
        "    return loss, accuracy\n",
        "\n",
        "def _report_on_batch(model, test_data_loader):\n",
        "    loss, accuracy = _calculate_reporting_metrics(model, test_data_loader)\n",
        "    print(\"loss =\", loss.item())\n",
        "    print(\"accuracy =\", accuracy.item())\n",
        "    print(\"\")\n",
        "\n",
        "def _report_on_epoch(model, test_data_loader, epoch_i):\n",
        "    print(f\"After {epoch_i + 1} epoch(s):\")\n",
        "    loss, accuracy = _calculate_reporting_metrics(model, test_data_loader)\n",
        "    print(\"  loss =\", loss.item())\n",
        "    print(\"  accuracy =\", accuracy.item())\n",
        "\n",
        "def _train_for_n_batches(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    test_data_loader,\n",
        "    optimizer,\n",
        "    batches_to_run,\n",
        "    verbose,\n",
        "    print_every\n",
        "):\n",
        "    batches_run = 0\n",
        "    for i in range(batches_to_run):\n",
        "        X, y = _next_X_y(train_data_loader)\n",
        "        _train_on_batch(model, X, y, optimizer)\n",
        "        batches_run += 1\n",
        "\n",
        "        # Reporting\n",
        "        if verbose and batches_run % print_every == 0:\n",
        "            _report_on_batch(model, test_data_loader)\n",
        "\n",
        "def _train_for_n_epoches(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    test_data_loader,\n",
        "    optimizer,\n",
        "    epochs_to_run,\n",
        "    verbose,\n",
        "    print_every\n",
        "):\n",
        "    global DEVICE\n",
        "    batches_run = 0\n",
        "    for i in range(epochs_to_run):\n",
        "        # For each batch:\n",
        "        for X, y in train_data_loader:\n",
        "            X = X.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            _train_on_batch(model, X, y, optimizer)\n",
        "            batches_run += 1\n",
        "\n",
        "            # Reporting\n",
        "            if verbose and batches_run % print_every == 0:\n",
        "                _report_on_batch(model, test_data_loader)\n",
        "\n",
        "        # Print after every epoch\n",
        "        if i == 0:\n",
        "            print(\"=\" * 20)\n",
        "        _report_on_epoch(model, test_data_loader, i)\n",
        "\n",
        "    print(\"=\" * 20)\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_data,\n",
        "    test_data,\n",
        "    optimizer=\"Adam\",\n",
        "    epochs_to_run=None,  # Train for 1 epoch if no training limit is given\n",
        "    batches_to_run=None,\n",
        "    batch_size=128,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=False,\n",
        "    print_every=100,\n",
        "    **kwargs\n",
        "):\n",
        "    # Initialize train /test DataLoaders:\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "    test_data_loader = torch.utils.data.DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size * 16,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Initialize Optimizer:\n",
        "    if type(optimizer) == str:\n",
        "        assert(optimizer in torch.optim.__dict__), (\n",
        "            \"optimizer must be one of the optimizers available in the \" +\n",
        "            \"torch.optim module, e.g. 'Adam'\"\n",
        "        )\n",
        "        optimizer = torch.optim.__dict__[optimizer]\n",
        "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if not epochs_to_run and not batches_to_run:\n",
        "        epochs_to_run = 1\n",
        "\n",
        "    if epochs_to_run:\n",
        "        _train_for_n_epoches(\n",
        "            model,\n",
        "            train_data_loader,\n",
        "            test_data_loader,\n",
        "            optimizer,\n",
        "            epochs_to_run,\n",
        "            verbose,\n",
        "            print_every\n",
        "        )\n",
        "    else:  # if batches_to_run:\n",
        "        _train_for_n_batches(\n",
        "            model,\n",
        "            train_data_loader,\n",
        "            test_data_loader,\n",
        "            optimizer,\n",
        "            batches_to_run,\n",
        "            verbose,\n",
        "            print_every\n",
        "        )\n",
        "\n",
        "def test(\n",
        "        model,\n",
        "        test_data,\n",
        "        adv_algorithm=\"PGD\",\n",
        "        eps=8/255,\n",
        "        alpha=2/255,\n",
        "        steps=4,\n",
        "        num_test_batches=10,\n",
        "        **kwargs\n",
        "    ):\n",
        "    global DEVICE\n",
        "    if type(adv_algorithm) == str:\n",
        "        assert(adv_algorithm in torchattacks.__dict__), (\n",
        "            \"adv_algorithm must be one of the algorithms available in the \" +\n",
        "            \"torchattacks package, e.g. 'PGD'\"\n",
        "        )\n",
        "        adv_algorithm = torchattacks.__dict__[adv_algorithm]\n",
        "\n",
        "    test_data_loader = torch.utils.data.DataLoader(\n",
        "        test_data,\n",
        "        batch_size=len(test_data)//num_test_batches\n",
        "    )\n",
        "\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    total_adv_loss = 0\n",
        "    total_adv_accuracy = 0\n",
        "\n",
        "    model.eval()\n",
        "    for X, y in test_data_loader:\n",
        "        X = X.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        logits, loss = model(X, y)\n",
        "        accuracy = _calculate_accuracy_from_logits(logits, y)\n",
        "\n",
        "        atk = adv_algorithm(model, eps=eps, alpha=alpha, steps=steps)\n",
        "        adv_images = atk(X, y).to(DEVICE)\n",
        "\n",
        "        adv_logits, adv_loss = model(adv_images, y)\n",
        "        adv_accuracy = _calculate_accuracy_from_logits(adv_logits, y)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "        total_adv_loss += adv_loss.item()\n",
        "        total_adv_accuracy += adv_accuracy.item()\n",
        "\n",
        "    # Average over the test batches to get final loss, accuracy, etc.\n",
        "    loss = total_loss / num_test_batches\n",
        "    accuracy = total_accuracy / num_test_batches\n",
        "    adv_loss = total_adv_loss / num_test_batches\n",
        "    adv_accuracy = total_adv_accuracy / num_test_batches\n",
        "\n",
        "    print(\"Final result:\")\n",
        "    print(\"  loss =\", loss)\n",
        "    print(\"  accuracy =\", accuracy)\n",
        "    print(\"  adv_loss =\", adv_loss)\n",
        "    print(\"  adv_accuracy =\", adv_accuracy)\n",
        "    print(\"=\" * 20)\n",
        "\n",
        "    results = dict(\n",
        "        loss = loss,\n",
        "        accuracy = accuracy,\n",
        "        adv_loss = adv_loss,\n",
        "        adv_accuracy = adv_accuracy\n",
        "    )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment Running Helpers"
      ],
      "metadata": {
        "id": "Qsziw82lXRiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentRun():\n",
        "    def __init__(self, experiment_params):\n",
        "        self.experiment_params = experiment_params\n",
        "        self.trained_by = USER\n",
        "\n",
        "        # Run the experiment:\n",
        "        self.start_time = time()\n",
        "        self.initialize_model()\n",
        "        self.train()\n",
        "        self.test()\n",
        "\n",
        "    def initialize_model(self):\n",
        "        assert(self.experiment_params[\"model_class\"] in globals()), (\n",
        "            \"model_class must be a pytorch Module class loaded into \" +\n",
        "            \"globals(), e.g. 'CNN'\"\n",
        "        )\n",
        "        model_class = globals()[self.experiment_params[\"model_class\"]]\n",
        "\n",
        "        torch.manual_seed(self.experiment_params[\"random_seed\"])\n",
        "        self.model = model_class(**self.experiment_params).to(DEVICE)\n",
        "\n",
        "    def train(self):\n",
        "        self.train_start_time = time()\n",
        "        train(self.model, **self.experiment_params)\n",
        "        self.train_end_time = time()\n",
        "\n",
        "    def test(self):\n",
        "        self.results = test(self.model, **self.experiment_params)\n",
        "        self.end_time = time()\n",
        "        print(\n",
        "            f\"({round((self.train_end_time - self.train_start_time)/60, 2)} min. training;\",\n",
        "            f\"{round((self.end_time - self.start_time)/60, 2)} min. total)\\n\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def state(self):\n",
        "        return {\n",
        "            \"trained_by\": self.trained_by,\n",
        "            \"start_time\": self.start_time,\n",
        "            \"train_start_time\": self.train_start_time,\n",
        "            \"train_end_time\": self.train_end_time,\n",
        "            \"end_time\": self.end_time,\n",
        "            **{key: value for key, value in self.experiment_params.items()\n",
        "                if key not in [\"train_data\", \"test_data\"]},\n",
        "            **self.results\n",
        "        }"
      ],
      "metadata": {
        "id": "ybTxwLTMRGYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_hashable(l):\n",
        "    l = deepcopy(l)\n",
        "    for i, item in enumerate(l):\n",
        "        if isinstance(item, dict):\n",
        "            l[i] = dict_to_hashable(item)\n",
        "        elif isinstance(item, list):\n",
        "            l[i] = list_to_hashable(item)\n",
        "    return tuple(l)\n",
        "\n",
        "def dict_to_hashable(d):\n",
        "    d = deepcopy(d)\n",
        "    for key in d.keys():\n",
        "        if isinstance(d[key], dict):\n",
        "            d[key] = dict_to_hashable(d[key])\n",
        "        elif isinstance(d[key], list):\n",
        "            d[key] = list_to_hashable(d[key])\n",
        "    return tuple(sorted(d.items()))\n",
        "\n",
        "class ExperimentStore():\n",
        "    def __init__(self, data_path=TRAINING_RUNS_PATH):\n",
        "        self.data_path = data_path\n",
        "        self._hash_keys = {\n",
        "            \"VisionTransformer\": {\n",
        "                key for key in VISION_TRANSFORMER_BASELINE.keys()\n",
        "                if key not in [\"train_data\", \"test_data\"]\n",
        "            },\n",
        "            \"CNN\": {\n",
        "                key for key in CNN_BASELINE.keys()\n",
        "                if key not in [\"train_data\", \"test_data\"]\n",
        "            }\n",
        "        }\n",
        "        self.state_data = self.load_state_data()\n",
        "\n",
        "    def _load_raw_state_data(self, verbose=True):\n",
        "        all_state_data_path_files = os.listdir(self.data_path)\n",
        "        state_data_files = [\n",
        "            file for file in all_state_data_path_files\n",
        "            if file.endswith(\".json\")\n",
        "        ]\n",
        "\n",
        "        if verbose:\n",
        "            print(\"# state files detected =\", len(state_data_files))\n",
        "            print(\"Loading...\")\n",
        "\n",
        "        data = []\n",
        "        for file_name in state_data_files:\n",
        "            with open(self.data_path + file_name, \"r\") as file:\n",
        "                data.append(json.load(file))\n",
        "\n",
        "        if verbose:\n",
        "            print(\"# Loaded =\", len(data))\n",
        "\n",
        "        return data\n",
        "\n",
        "    def hash_keys(self, model_class):\n",
        "        assert(model_class in [\"VisionTransformer\", \"CNN\"])\n",
        "        return self._hash_keys[model_class]\n",
        "\n",
        "    def _raw_state_to_hashable(self, raw_state):\n",
        "        assert(\"model_class\" in raw_state)\n",
        "        hash_keys = self.hash_keys(raw_state[\"model_class\"])\n",
        "        hash_state = {\n",
        "            key: value for key, value in raw_state.items()\n",
        "            if key in hash_keys\n",
        "        }\n",
        "        assert len(hash_state) == len(hash_keys)\n",
        "        return dict_to_hashable(hash_state)\n",
        "\n",
        "    def _raw_state_data_to_dict(self, raw_state_data):\n",
        "        return {\n",
        "            self._raw_state_to_hashable(raw_state): raw_state\n",
        "            for raw_state in raw_state_data\n",
        "        }\n",
        "\n",
        "    def load_state_data(self):\n",
        "        return self._raw_state_data_to_dict(self._load_raw_state_data())\n",
        "\n",
        "    def contains(self, raw_state):\n",
        "        key = self._raw_state_to_hashable(raw_state)\n",
        "        return key in self.state_data\n",
        "\n",
        "    def get(self, raw_state):\n",
        "        key = self._raw_state_to_hashable(raw_state)\n",
        "        return self.state_data[key]\n",
        "\n",
        "    def add(self, raw_state):\n",
        "        key = self._raw_state_to_hashable(raw_state)\n",
        "        self.state_data[key] = raw_state\n",
        "\n",
        "    def save_to_file(self, state, file_path):\n",
        "        assert(file_path.endswith(\".json\"))\n",
        "        if \"/\" not in file_path:\n",
        "            file_path = self.data_path + file_path\n",
        "\n",
        "        print(\"Saving...\")\n",
        "        with open(file_path, \"w\") as file:\n",
        "            json.dump(state, file)\n",
        "        print(\"Saved to\", file_path)\n",
        "\n",
        "    def save(self, state, save_to_file=\"\"):\n",
        "        if not self.contains(state):\n",
        "            print(\"Saving state...\")\n",
        "            self.add(state)\n",
        "\n",
        "        if save_to_file:\n",
        "            self.save_to_file(state, save_to_file)\n",
        "\n",
        "    def df(self, cleaning_function=lambda df: df):\n",
        "        df = pd.DataFrame(self.state_data.values())\n",
        "        # Clean df and return\n",
        "        return cleaning_function(df)\n"
      ],
      "metadata": {
        "id": "CTY8NYSBsNTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Experiment():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_class,\n",
        "        experiment_store,\n",
        "        save_to_file_format=\"{trained_by}-{end_time:.0f}.json\",\n",
        "        **kwargs  # kwargs keys = param and values = iterable of values to test\n",
        "    ):\n",
        "        assert(model_class in [\"VisionTransformer\", \"CNN\"])\n",
        "        self.model_class = model_class\n",
        "        self.vars = kwargs\n",
        "        self.experiment_on_string = \"+\".join(sorted(self.vars.keys()))\n",
        "\n",
        "        self.experiment_store = experiment_store\n",
        "        self.save_to_file_format = save_to_file_format\n",
        "        self.data = []\n",
        "\n",
        "        # Actually run the experiment\n",
        "        self.run()\n",
        "\n",
        "    def _run_experiment_and_save(self, experiment_params):\n",
        "        if self.experiment_store.contains(experiment_params):\n",
        "            print(\"Loading saved experiment values...\")\n",
        "            state = self.experiment_store.get(experiment_params)\n",
        "        else:\n",
        "            # Perform the complete experiment training / testing run:\n",
        "            state = ExperimentRun(experiment_params).state\n",
        "\n",
        "            # Save results locally and to file:\n",
        "            save_to_file = self.save_to_file_format.format(**state)\n",
        "            self.experiment_store.save(state, save_to_file)\n",
        "\n",
        "        self.data.append(state)\n",
        "\n",
        "    def run(self):\n",
        "        if len(self.vars) == 1:  # Univariate experiment\n",
        "            for var, allowed_values in self.vars.items():  # Only runs once\n",
        "                experiments = [{var: value} for value in allowed_values]\n",
        "\n",
        "        else:  # Multivariate experiment\n",
        "            # Sort vars for consistent order:\n",
        "            vars = sorted(self.vars.items())\n",
        "            # Unzip into the variable id and allowed values:\n",
        "            vars, allowed = zip(*vars)\n",
        "            # Calculate every possible combination of the allowed values. Each\n",
        "            # of these is one experiment of the form:\n",
        "                # {\"var1\": var1_val_1, \"var2\", var2_val_1, ...}\n",
        "            experiments = [\n",
        "                dict(zip(vars, combo))\n",
        "                for combo in itertools.product(*allowed)\n",
        "            ]\n",
        "\n",
        "        if self.model_class == \"VisionTransformer\":\n",
        "            baseline_params = VISION_TRANSFORMER_BASELINE\n",
        "        elif self.model_class == \"CNN\":\n",
        "            baseline_params = CNN_BASELINE\n",
        "\n",
        "        for experiment_params in experiments:\n",
        "            print(f\"{self.model_class}:\", experiment_params)\n",
        "            experiment_params = {**baseline_params, **experiment_params}\n",
        "            self._run_experiment_and_save(experiment_params)\n",
        "\n",
        "    def df(self, cleaning_function=lambda df: df):\n",
        "        df = pd.DataFrame(self.data)\n",
        "        # Clean df and return\n",
        "        df[\"safety_capability_ratio\"] = df[\"adv_accuracy\"] / df[\"accuracy\"]\n",
        "        return cleaning_function(df)\n"
      ],
      "metadata": {
        "id": "xz_0xy_ytsdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting Helpers"
      ],
      "metadata": {
        "id": "CLO_tDK9qbd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PARAM_NAMES = dict(\n",
        "    # -------------\n",
        "    # Model Params:\n",
        "    # -------------\n",
        "    # Architecture Size Params:\n",
        "    patch_size=\"Image Patch Size\",\n",
        "    model_dim=\"Primary Model Dimension\",\n",
        "    head_dim=\"Attention Head Dimension\",\n",
        "    num_heads=\"Number of Attention Heads\",\n",
        "    mlp_dim=\"MLP Dimension\",\n",
        "    num_layers=\"Number of Layers\",\n",
        "    # Other Architecture Params:\n",
        "    position_embedding=\"Positional Embedding\",\n",
        "    QKV_bias=\"Q, K, V Calculated with Bias Term\",\n",
        "    scaling_factor=\"Attention Scaling Factor\",\n",
        "    activation_function=\"MLP Activation Function\",\n",
        "    # Regularization Params:\n",
        "    layer_norm_patches=\"Patches Layer Normalized\",\n",
        "    layer_norm=\"Layer Normalization\",\n",
        "    dropout=\"Dropout Rate\",\n",
        "    # ----------------\n",
        "    # Training Params:\n",
        "    # ----------------\n",
        "    epochs_to_run=\"Epochs to Run\",\n",
        "    batch_size=\"Batch Size\",\n",
        "    learning_rate=\"Learning Rate\",\n",
        "    # -----------------------------\n",
        "    # Testing / Adversarial Params:\n",
        "    # -----------------------------\n",
        "    eps=\"Adversarial Budget\",\n",
        "    alpha=\"Adversarial Step Size\",\n",
        "    steps=\"Adversarial Steps\",\n",
        "    # --------------------------\n",
        "    # Other CNN-specific Params:\n",
        "    # --------------------------\n",
        "    conv_layer_1=\"1st Convolutional Layer Size\",\n",
        "    conv_layer_2=\"2nd Convolutional Layer Size\",\n",
        "    conv_layer_3=\"3rd Convolutional Layer Size\",\n",
        "    conv_kernel=\"Convolutional Kernel Size\",\n",
        "    conv_stride=\"Convolutional Stride\",\n",
        "    conv_padding=\"Convolutional Padding\",\n",
        "    max_pool_kernel=\"Max Pooling Kernel Size\",\n",
        "    fully_connected_layer_1=\"1st Fully Connected Layer Size\",\n",
        "    conv_normalization=\"Normalization on Convolutional Layers\"\n",
        ")\n",
        "\n",
        "def to_percents(nums):\n",
        "    return [\"%d%%\" % (num * 100) for num in nums]\n",
        "\n",
        "def _get_raw_plot_data(\n",
        "    experiment,\n",
        "    var,\n",
        "    baseline_value=None,\n",
        "    metrics=[\"accuracy\", \"adv_accuracy\", \"safety_capability_ratio\"]\n",
        "):\n",
        "    # Assemble the plot data:\n",
        "    df = experiment.df()\n",
        "    df = df.melt(\n",
        "        id_vars=[\"random_seed\", var],\n",
        "        value_vars=metrics,\n",
        "        var_name=\"metric\",\n",
        "        value_name=\"value\"\n",
        "    ).groupby([var, \"metric\"]).mean()[\"value\"].reset_index()\n",
        "\n",
        "    var_name = PARAM_NAMES[var]\n",
        "    x_label = var_name\n",
        "\n",
        "    model_class = experiment.model_class\n",
        "    if model_class == \"VisionTransformer\":\n",
        "        baseline_params = VISION_TRANSFORMER_BASELINE\n",
        "    elif model_class == \"CNN\":\n",
        "        baseline_params = CNN_BASELINE\n",
        "    else:\n",
        "        assert(baseline_value)\n",
        "\n",
        "    if not baseline_value:\n",
        "        baseline_value = baseline_params[var]\n",
        "\n",
        "    return df, var_name, x_label, baseline_value\n",
        "\n",
        "def plot_line_chart(\n",
        "    experiment,\n",
        "    var,\n",
        "    baseline_value=None,\n",
        "    hide_legend=False,\n",
        "    var_is_percent=False,\n",
        "    log_var=False,\n",
        "    metrics=[\"accuracy\", \"adv_accuracy\", \"safety_capability_ratio\"],\n",
        "    title=None\n",
        "):\n",
        "    # Gather data for plot:\n",
        "    df, var_name, x_label, baseline_value =\\\n",
        "        _get_raw_plot_data(experiment, var, baseline_value, metrics)\n",
        "\n",
        "    # Handle log-scale variables:\n",
        "    if log_var:\n",
        "        df[var] = np.log10(df[var])\n",
        "        x_label = x_label + \" (10^X)\"\n",
        "        baseline_value = np.log10(baseline_value)\n",
        "\n",
        "    # To be accurate, this needs to happen after the log calculation:\n",
        "    x_range = df[var].max() - df[var].min()\n",
        "\n",
        "    metric_labels = {\n",
        "        \"accuracy\": \"Accuracy\",\n",
        "        \"adv_accuracy\": \"Adversarial Accuracy\",\n",
        "        \"safety_capability_ratio\": \"Safety Capability Ratio\"\n",
        "    }\n",
        "    metric_labels = [metric_labels[metric] for metric in metrics]\n",
        "\n",
        "    # Plot the chart:\n",
        "    gg = (\n",
        "        ggplot(df, aes(x=var, y=\"value\", color=\"metric\"))\n",
        "            + geom_line(size=1)\n",
        "            + geom_vline(\n",
        "                aes(xintercept=baseline_value),\n",
        "                linetype=\"dashed\",\n",
        "                color=\"black\"\n",
        "            )\n",
        "            + annotate(\n",
        "                \"text\",\n",
        "                x=baseline_value,\n",
        "                y=1,\n",
        "                label=\"Baseline\",\n",
        "                ha=\"left\",\n",
        "                va=\"bottom\",\n",
        "                nudge_x=x_range * 0.01,\n",
        "                size=9,\n",
        "                color=\"black\"\n",
        "            )\n",
        "            + theme_minimal()\n",
        "            + ggtitle(var_name if title is None else title)\n",
        "            + xlab(x_label)\n",
        "            + ylab(\"\")\n",
        "            + scale_y_continuous(labels=to_percents)\n",
        "            + scale_color_manual(\n",
        "                name=\" \",\n",
        "                values={\n",
        "                    \"accuracy\": \"blue\",\n",
        "                    \"adv_accuracy\": \"orange\",\n",
        "                    \"safety_capability_ratio\": \"green\"\n",
        "                },\n",
        "                labels=metric_labels\n",
        "            )\n",
        "            + theme(\n",
        "                plot_title=element_text(weight=\"bold\"),\n",
        "                axis_title=element_text(size=10)\n",
        "            )\n",
        "    )\n",
        "    if hide_legend:\n",
        "        gg += theme(legend_position=\"none\")\n",
        "\n",
        "    if var_is_percent:\n",
        "        gg += scale_x_continuous(labels=to_percents)\n",
        "\n",
        "    return gg\n",
        "\n",
        "def plot_bar_chart(\n",
        "    experiment,\n",
        "    var,\n",
        "    baseline_value=None,\n",
        "    hide_legend=False,\n",
        "    rotate_x_axis_labels=False,\n",
        "    metrics=[\"accuracy\", \"adv_accuracy\", \"safety_capability_ratio\"],\n",
        "    title=None\n",
        "):\n",
        "    # Gather data for plot:\n",
        "    df, var_name, x_label, baseline_value =\\\n",
        "        _get_raw_plot_data(experiment, var, baseline_value, metrics)\n",
        "\n",
        "    # Baseline label:\n",
        "    df[var] = df[var].apply(\n",
        "        lambda val: str(val) + \"\\n(Baseline)\" if val == baseline_value else str(val)\n",
        "    )\n",
        "\n",
        "    metric_labels = {\n",
        "        \"accuracy\": \"Accuracy\",\n",
        "        \"adv_accuracy\": \"Adversarial Accuracy\",\n",
        "        \"safety_capability_ratio\": \"Safety Capability Ratio\"\n",
        "    }\n",
        "    metric_labels = [metric_labels[metric] for metric in metrics]\n",
        "\n",
        "    # Plot the chart:\n",
        "    gg = (\n",
        "        ggplot(df, aes(x=var, y=\"value\", fill=\"metric\"))\n",
        "            + geom_bar(position=\"dodge\", stat=\"identity\")\n",
        "            + theme_minimal()\n",
        "            + ggtitle(var_name if title is None else title)\n",
        "            + xlab(x_label)\n",
        "            + ylab(\"\")\n",
        "            + scale_y_continuous(labels=to_percents)\n",
        "            + scale_fill_manual(\n",
        "                name=\" \",\n",
        "                values={\n",
        "                    \"accuracy\": \"blue\",\n",
        "                    \"adv_accuracy\": \"darkorange\",\n",
        "                    \"safety_capability_ratio\": \"green\"\n",
        "                },\n",
        "                labels=metric_labels\n",
        "            )\n",
        "            + theme(\n",
        "                plot_title=element_text(weight=\"bold\"),\n",
        "                axis_title=element_text(size=10)\n",
        "            )\n",
        "    )\n",
        "    if hide_legend:\n",
        "        gg += theme(legend_position=\"none\")\n",
        "    if rotate_x_axis_labels:\n",
        "        gg += theme(axis_text_x=element_text(angle=90, vjust=1, hjust=0.5))\n",
        "\n",
        "    return gg\n"
      ],
      "metadata": {
        "id": "Iav1wQPcagvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Experiments"
      ],
      "metadata": {
        "id": "JF7QuRqRpANJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_store = ExperimentStore()"
      ],
      "metadata": {
        "id": "HsjLuHlD7PHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision Transformer"
      ],
      "metadata": {
        "id": "n-VYMPxFZLnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shared_vision_transformers_experiment_args = dict(\n",
        "    model_class=\"VisionTransformer\",\n",
        "    experiment_store=experiment_store\n",
        ")\n",
        "\n",
        "# ============\n",
        "# Experiments:\n",
        "# ============\n",
        "# WARNING: This will be very slow to run if these experiments aren't actaully\n",
        "# already complete. If they are it will be fast and just load their results.\n",
        "\n",
        "completed_transformer_experiments = dict(\n",
        "    layer_norm_patches = Experiment(\n",
        "        layer_norm_patches=[False, True],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    patch_size = Experiment(\n",
        "        patch_size=[2, 4, 7, 14, 28],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    model_dim = Experiment(\n",
        "        model_dim=range(4, 64 + 1, 4),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    head_dim = Experiment(\n",
        "        head_dim=range(4, 64 + 1, 4),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    num_heads = Experiment(\n",
        "        num_heads=range(1, 16 + 1),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    mlp_dim = Experiment(\n",
        "        mlp_dim=range(4, 128 + 1, 4),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    num_layers = Experiment(\n",
        "        num_layers=range(1, 8 + 1),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    position_embedding = Experiment(\n",
        "        position_embedding=[\"learned\", \"standard_transformer\", \"none\"],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    QKV_bias = Experiment(\n",
        "        QKV_bias=[True, False],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    scaling_factor = Experiment(\n",
        "        scaling_factor=[\n",
        "            None, 1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2,\n",
        "            1, 2, 4, 8, 16, 32, 64, 128\n",
        "        ],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    activation_function = Experiment(\n",
        "        activation_function=[\n",
        "            \"ReLU\", \"Sigmoid\", \"GELU\", \"Tanh\", \"LeakyReLU\", \"LogSigmoid\"\n",
        "        ],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    layer_norm = Experiment(\n",
        "        layer_norm=[\"before\", \"after\", \"both\", \"none\"],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    dropout = Experiment(\n",
        "        dropout=[n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    epochs_to_run = Experiment(\n",
        "        epochs_to_run=range(1, 21),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    batch_size = Experiment(\n",
        "        batch_size=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1028, 2048],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    learning_rate = Experiment(\n",
        "        learning_rate=[1*10**(-i) for i in range(7)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    eps = Experiment(\n",
        "        eps=[8/255] + [n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    alpha = Experiment(\n",
        "        alpha=[2/255] + [n / 100 for n in range(0, 50 + 1, 2)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    steps = Experiment(\n",
        "        steps=range(1, 21),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "qMwRhuzg9IoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "uLv7055cZXKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shared_cnn_experiment_args = dict(\n",
        "    model_class=\"CNN\",\n",
        "    experiment_store=experiment_store\n",
        ")\n",
        "\n",
        "# ============\n",
        "# Experiments:\n",
        "# ============\n",
        "# WARNING: This will be very slow to run if these experiments aren't actaully\n",
        "# already complete. If they are it will be fast and just load their results.\n",
        "\n",
        "completed_cnn_experiments = dict(\n",
        "    conv_layer_1 = Experiment(\n",
        "        conv_layer_1=range(4, 64 + 1, 4),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_layer_2 = Experiment(\n",
        "        conv_layer_2=range(8, 128 + 1, 8),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_layer_3 = Experiment(\n",
        "        conv_layer_3=range(0, 256 + 1, 16),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_kernel = Experiment(\n",
        "        conv_kernel=range(2, 11 + 1),  # 11 = Max kernel size where baseline has no issues\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_stride = Experiment(\n",
        "        conv_stride=range(1, 3 + 1),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    fully_connected_layer_1 = Experiment(\n",
        "        fully_connected_layer_1=range(0, 128 + 1, 8),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    activation_function = Experiment(\n",
        "        activation_function=[\n",
        "            \"ReLU\", \"Sigmoid\", \"GELU\", \"Tanh\", \"LeakyReLU\", \"LogSigmoid\"\n",
        "        ],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_normalization = Experiment(\n",
        "        conv_normalization=[\"none\", \"batch\", \"layer\"],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    dropout = Experiment(\n",
        "        dropout=[n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    epochs_to_run = Experiment(\n",
        "        epochs_to_run=range(1, 21),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    batch_size = Experiment(\n",
        "        batch_size=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1028, 2048],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    learning_rate = Experiment(\n",
        "        learning_rate=[1*10**(-i) for i in range(7)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    eps = Experiment(\n",
        "        eps=[8/255] + [n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    alpha = Experiment(\n",
        "        alpha=[2/255] + [n / 100 for n in range(0, 50 + 1, 2)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    steps = Experiment(\n",
        "        steps=range(1, 21),\n",
        "        **shared_cnn_experiment_args\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "tdKOhCqTZWTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Results"
      ],
      "metadata": {
        "id": "YpFltdjDpZot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading All Completed Experiments"
      ],
      "metadata": {
        "id": "msTLksKmAX8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shared_vision_transformers_experiment_args = dict(\n",
        "    model_class=\"VisionTransformer\",\n",
        "    random_seed=COMPLETED_RANDOM_SEEDS,\n",
        "    experiment_store=experiment_store\n",
        ")\n",
        "\n",
        "shared_cnn_experiment_args = dict(\n",
        "    model_class=\"CNN\",\n",
        "    random_seed=COMPLETED_RANDOM_SEEDS,\n",
        "    experiment_store=experiment_store\n",
        ")\n",
        "\n",
        "completed_transformer_experiments = dict(\n",
        "    layer_norm_patches = Experiment(\n",
        "        layer_norm_patches=[False, True],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    patch_size = Experiment(\n",
        "        patch_size=[2, 4, 7, 14, 28],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    model_dim = Experiment(\n",
        "        model_dim=range(4, 64 + 1, 4),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    head_dim = Experiment(\n",
        "        head_dim=range(4, 64 + 1, 4),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    num_heads = Experiment(\n",
        "        num_heads=range(1, 16 + 1),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    mlp_dim = Experiment(\n",
        "        mlp_dim=range(4, 128 + 1, 4),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    num_layers = Experiment(\n",
        "        num_layers=range(1, 8 + 1),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    position_embedding = Experiment(\n",
        "        position_embedding=[\"learned\", \"standard_transformer\", \"none\"],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    QKV_bias = Experiment(\n",
        "        QKV_bias=[True, False],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    scaling_factor = Experiment(\n",
        "        scaling_factor=[\n",
        "            None, 1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2,\n",
        "            1, 2, 4, 8, 16, 32, 64, 128\n",
        "        ],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    activation_function = Experiment(\n",
        "        activation_function=[\n",
        "            \"ReLU\", \"Sigmoid\", \"GELU\", \"Tanh\", \"LeakyReLU\", \"LogSigmoid\"\n",
        "        ],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    layer_norm = Experiment(\n",
        "        layer_norm=[\"before\", \"after\", \"both\", \"none\"],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    dropout = Experiment(\n",
        "        dropout=[n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    epochs_to_run = Experiment(\n",
        "        epochs_to_run=range(1, 21),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    batch_size = Experiment(\n",
        "        batch_size=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1028, 2048],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    learning_rate = Experiment(\n",
        "        learning_rate=[1*10**(-i) for i in range(7)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    eps = Experiment(\n",
        "        eps=[8/255] + [n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    alpha = Experiment(\n",
        "        alpha=[2/255] + [n / 100 for n in range(0, 50 + 1, 2)],\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    ),\n",
        "    steps = Experiment(\n",
        "        steps=range(1, 21),\n",
        "        **shared_vision_transformers_experiment_args\n",
        "    )\n",
        ")\n",
        "\n",
        "completed_cnn_experiments = dict(\n",
        "    conv_layer_1 = Experiment(\n",
        "        conv_layer_1=range(4, 64 + 1, 4),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_layer_2 = Experiment(\n",
        "        conv_layer_2=range(8, 128 + 1, 8),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_layer_3 = Experiment(\n",
        "        conv_layer_3=range(0, 256 + 1, 16),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_kernel = Experiment(\n",
        "        conv_kernel=range(2, 11 + 1),  # 11 = Max kernel size where baseline has no issues\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_stride = Experiment(\n",
        "        conv_stride=range(1, 3 + 1),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    fully_connected_layer_1 = Experiment(\n",
        "        fully_connected_layer_1=range(0, 128 + 1, 8),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    activation_function = Experiment(\n",
        "        activation_function=[\n",
        "            \"ReLU\", \"Sigmoid\", \"GELU\", \"Tanh\", \"LeakyReLU\", \"LogSigmoid\"\n",
        "        ],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    conv_normalization = Experiment(\n",
        "        conv_normalization=[\"none\", \"batch\", \"layer\"],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    dropout = Experiment(\n",
        "        dropout=[n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    epochs_to_run = Experiment(\n",
        "        epochs_to_run=range(1, 21),\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    batch_size = Experiment(\n",
        "        batch_size=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1028, 2048],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    learning_rate = Experiment(\n",
        "        learning_rate=[1*10**(-i) for i in range(7)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    eps = Experiment(\n",
        "        eps=[8/255] + [n / 100 for n in range(0, 100 + 1, 5)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    alpha = Experiment(\n",
        "        alpha=[2/255] + [n / 100 for n in range(0, 50 + 1, 2)],\n",
        "        **shared_cnn_experiment_args\n",
        "    ),\n",
        "    steps = Experiment(\n",
        "        steps=range(1, 21),\n",
        "        **shared_cnn_experiment_args\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "wCfqxJjHAWku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual Plots"
      ],
      "metadata": {
        "id": "AofZdBuRrdbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vars_to_plot = [\n",
        "    \"patch_size\",\n",
        "    \"model_dim\",\n",
        "    \"head_dim\",\n",
        "    \"num_heads\",\n",
        "    \"mlp_dim\",\n",
        "    \"num_layers\",\n",
        "    \"scaling_factor\"\n",
        "]\n",
        "for var in vars_to_plot:\n",
        "    print(plot_line_chart(\n",
        "        completed_transformer_experiments[var],\n",
        "        var,\n",
        "        baseline_value=1/np.sqrt(8) if var == \"scaling_factor\" else None,\n",
        "        log_var=(var == \"scaling_factor\")\n",
        "    ))"
      ],
      "metadata": {
        "id": "9plchJhjg48m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(plot_line_chart(\n",
        "    completed_transformer_experiments[\"dropout\"],\n",
        "    \"dropout\",\n",
        "    var_is_percent=True\n",
        "))"
      ],
      "metadata": {
        "id": "Vdv9hFSNhGgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for var in [\"epochs_to_run\", \"batch_size\", \"learning_rate\"]:\n",
        "    print(plot_line_chart(\n",
        "        completed_transformer_experiments[var],\n",
        "        var,\n",
        "        log_var=(var == \"learning_rate\")\n",
        "    ))"
      ],
      "metadata": {
        "id": "yPNbYCeUfWv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for var in [\"eps\", \"alpha\", \"steps\"]:\n",
        "    print(plot_line_chart(\n",
        "        completed_transformer_experiments[var],\n",
        "        var,\n",
        "        metrics=[\"accuracy\", \"adv_accuracy\"]\n",
        "    ))"
      ],
      "metadata": {
        "id": "v2_VAG8hZv0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar_chart_vars = [\n",
        "    \"position_embedding\",\n",
        "    \"QKV_bias\",\n",
        "    \"activation_function\",\n",
        "    \"layer_norm_patches\",\n",
        "    \"layer_norm\"\n",
        "]\n",
        "for var in bar_chart_vars:\n",
        "    print(plot_bar_chart(\n",
        "        completed_transformer_experiments[var],\n",
        "        var,\n",
        "        metrics=[\"accuracy\", \"adv_accuracy\"]\n",
        "    ))"
      ],
      "metadata": {
        "id": "BNr1iIYfniJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile Results Plots in One Big Summary Plot"
      ],
      "metadata": {
        "id": "-pwV70tIrXsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_summary_plot(\n",
        "    completed_experiments,\n",
        "    vars_to_plot,\n",
        "    bar_chart_vars,\n",
        "    plot_kwargs,\n",
        "    num_rows,\n",
        "    metrics=[\"accuracy\", \"adv_accuracy\", \"safety_capability_ratio\"]\n",
        "):\n",
        "    metric_labels = {\n",
        "        \"accuracy\": \"Accuracy\",\n",
        "        \"adv_accuracy\": \"Adversarial Accuracy\",\n",
        "        \"safety_capability_ratio\": \"Safety Capability Ratio\"\n",
        "    }\n",
        "    metric_labels = [metric_labels[metric] for metric in metrics]\n",
        "\n",
        "    legend_only = (\n",
        "        ggplot(\n",
        "            pd.DataFrame({\"metric\": metrics}),\n",
        "            aes(x=0, y=0, color=\"metric\")\n",
        "        )\n",
        "        + geom_line()\n",
        "        + scale_color_manual(\n",
        "            name=\" \",\n",
        "            values={\n",
        "                \"accuracy\": \"blue\",\n",
        "                \"adv_accuracy\": \"orange\",\n",
        "                \"safety_capability_ratio\": \"green\"\n",
        "            },\n",
        "            labels=metric_labels\n",
        "        )\n",
        "        + theme_void()\n",
        "        + theme(legend_position=(0.5, 0.5), legend_direction=\"vertical\")\n",
        "    )\n",
        "\n",
        "    blank_plot = ggplot() + theme_void()\n",
        "\n",
        "    plots = [\n",
        "        plot_line_chart(\n",
        "            completed_experiments[var],\n",
        "            var,\n",
        "            metrics=metrics,\n",
        "            hide_legend=True,\n",
        "            **plot_kwargs.get(var, {})\n",
        "        )\n",
        "        if var not in bar_chart_vars\n",
        "        else plot_bar_chart(\n",
        "            completed_experiments[var],\n",
        "            var,\n",
        "            metrics=metrics,\n",
        "            hide_legend=True,\n",
        "            **plot_kwargs.get(var, {})\n",
        "        )\n",
        "        for i, var in enumerate(vars_to_plot)\n",
        "    ] + [legend_only]\n",
        "    plots += [blank_plot for _ in range(len(plots) % num_rows)]\n",
        "\n",
        "    size_of_each_plot = (3, 3)\n",
        "    for i, row in enumerate(np.array_split(plots, num_rows)):\n",
        "        for j, gg in enumerate(row):\n",
        "            if j == 0:\n",
        "                combined_row = pw.load_ggplot(gg, figsize=size_of_each_plot)\n",
        "            else:\n",
        "                combined_row |= pw.load_ggplot(gg, figsize=size_of_each_plot)\n",
        "\n",
        "        if i == 0:\n",
        "            combined_plot = combined_row\n",
        "        else:\n",
        "            combined_plot /= combined_row\n",
        "\n",
        "    return combined_plot"
      ],
      "metadata": {
        "id": "zkesGVBVYw_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vision Transformer"
      ],
      "metadata": {
        "id": "oIPr7vPFmjcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vars_to_plot = [\n",
        "    \"patch_size\",\n",
        "    \"model_dim\",\n",
        "    \"head_dim\",\n",
        "    \"num_heads\",\n",
        "    \"mlp_dim\",\n",
        "    \"num_layers\",\n",
        "    \"position_embedding\",\n",
        "    \"QKV_bias\",\n",
        "    \"scaling_factor\",\n",
        "    \"activation_function\",\n",
        "    \"layer_norm_patches\",\n",
        "    \"layer_norm\",\n",
        "    \"dropout\",\n",
        "    \"epochs_to_run\",\n",
        "    \"batch_size\",\n",
        "    \"learning_rate\",\n",
        "    \"eps\",\n",
        "    \"alpha\",\n",
        "    \"steps\"\n",
        "]\n",
        "bar_chart_vars = [\n",
        "    \"position_embedding\",\n",
        "    \"QKV_bias\",\n",
        "    \"activation_function\",\n",
        "    \"layer_norm_patches\",\n",
        "    \"layer_norm\"\n",
        "]\n",
        "plot_kwargs = {\n",
        "    \"scaling_factor\": {\"baseline_value\": 1/np.sqrt(8), \"log_var\": True},\n",
        "    \"activation_function\": {\"rotate_x_axis_labels\": True},\n",
        "    \"dropout\": {\"var_is_percent\": True},\n",
        "    \"learning_rate\": {\"log_var\": True},\n",
        "    \"eps\": {\"var_is_percent\": True},\n",
        "    \"alpha\": {\"var_is_percent\": True}\n",
        "}\n",
        "\n",
        "combined_summary_plot(\n",
        "    completed_transformer_experiments,\n",
        "    vars_to_plot,\n",
        "    bar_chart_vars,\n",
        "    plot_kwargs,\n",
        "    metrics=[\"accuracy\", \"adv_accuracy\"],\n",
        "    num_rows=5\n",
        ")"
      ],
      "metadata": {
        "id": "QEBnxX4yfRpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN"
      ],
      "metadata": {
        "id": "bBAVrlVlmqzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vars_to_plot = [\n",
        "    \"conv_layer_1\",\n",
        "    \"conv_layer_2\",\n",
        "    \"conv_layer_3\",\n",
        "    \"conv_kernel\",\n",
        "    \"conv_stride\",\n",
        "    \"fully_connected_layer_1\",\n",
        "    \"activation_function\",\n",
        "    \"conv_normalization\",\n",
        "    \"dropout\",\n",
        "    \"epochs_to_run\",\n",
        "    \"batch_size\",\n",
        "    \"learning_rate\",\n",
        "    \"eps\",\n",
        "    \"alpha\",\n",
        "    \"steps\"\n",
        "]\n",
        "bar_chart_vars = [\n",
        "    \"activation_function\",\n",
        "    \"conv_normalization\"\n",
        "]\n",
        "plot_kwargs = {\n",
        "    \"activation_function\": {\n",
        "        \"rotate_x_axis_labels\": True,\n",
        "        \"title\": \"Activation Function\"\n",
        "    },\n",
        "    \"dropout\": {\"var_is_percent\": True},\n",
        "    \"learning_rate\": {\"log_var\": True},\n",
        "    \"eps\": {\"var_is_percent\": True},\n",
        "    \"alpha\": {\"var_is_percent\": True}\n",
        "}\n",
        "combined_summary_plot(\n",
        "    completed_cnn_experiments,\n",
        "    vars_to_plot,\n",
        "    bar_chart_vars,\n",
        "    plot_kwargs,\n",
        "    metrics=[\"accuracy\", \"adv_accuracy\"],\n",
        "    num_rows=4\n",
        ")"
      ],
      "metadata": {
        "id": "0Wq34J0IxVv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyI-Kyh5J0OT"
      },
      "source": [
        "# Supplementary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adversarial Image Exploration"
      ],
      "metadata": {
        "id": "oI35dxnxk-1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Train / Load a Trained Model"
      ],
      "metadata": {
        "id": "hgf7HGIqqjlT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFxShySiVjy2"
      },
      "outputs": [],
      "source": [
        "model = ExperimentRun(VISION_TRANSFORMER_BASELINE).model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Prepare Adversarial Images"
      ],
      "metadata": {
        "id": "I6WkJcwMqr7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K_DFNA1j75s"
      },
      "outputs": [],
      "source": [
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size=len(test_data)  # Test on all test data\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "X, y = next(iter(test_data_loader))\n",
        "X = X.to(DEVICE)\n",
        "y = y.to(DEVICE)\n",
        "\n",
        "atk = torchattacks.PGD(model, eps=8/255, alpha=2/255, steps=4)\n",
        "adv_images = atk(X, y)\n",
        "adv_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Evaluate Adversarial Robustness"
      ],
      "metadata": {
        "id": "L_MjmrrIqz2N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Wc3eIatYqv"
      },
      "outputs": [],
      "source": [
        "logits, loss = model(adv_images, y)\n",
        "accuracy = (F.softmax(logits, dim=1).argmax(dim=1) == y).float().mean()\n",
        "print(\"loss =\", loss.item())\n",
        "print(\"accuracy =\", accuracy.item())\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View Adversarial Images"
      ],
      "metadata": {
        "id": "hIe4M7KRqyRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfuAh8NntfsG"
      },
      "outputs": [],
      "source": [
        "def display_adversarial_attack_example(\n",
        "    model,\n",
        "    true_img,\n",
        "    adv_img,\n",
        "    true_label,\n",
        "    show_perturbation=True,\n",
        "    show_difference_diagnostics=True\n",
        "):\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    cols, rows = 2, 1\n",
        "    if show_perturbation:\n",
        "      cols = 3\n",
        "\n",
        "    predicted_true = model(true_img.view(1, 1, 28, 28)).argmax().item()\n",
        "    predicted_adv = model(adv_img.view(1, 1, 28, 28)).argmax().item()\n",
        "\n",
        "    # True image\n",
        "    current_col = 1\n",
        "    figure.add_subplot(rows, cols, current_col)\n",
        "    plt.title(f\"Orginal Image\\n{true_label} (predicted: {predicted_true})\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(true_img.squeeze().cpu(), cmap=\"gray\")\n",
        "\n",
        "    # Perturbation\n",
        "    if show_perturbation:\n",
        "        current_col += 1\n",
        "        figure.add_subplot(rows, cols, current_col)\n",
        "        plt.title(\"+ Perturbation =\\n\")\n",
        "        plt.axis(\"off\")\n",
        "        eps = 8/255\n",
        "        plt.imshow(\n",
        "            (adv_img - true_img).squeeze().cpu(),\n",
        "            cmap=LinearSegmentedColormap.from_list(\"rg\", [\"r\", \"w\", \"g\"], N=256),\n",
        "            vmin=-eps,\n",
        "            vmax=eps\n",
        "        )\n",
        "\n",
        "    # Adversarial image\n",
        "    current_col += 1\n",
        "    figure.add_subplot(rows, cols, current_col)\n",
        "    plt.title(f\"Adversarial Image\\n{true_label} (predicted: {predicted_adv})\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(adv_img.squeeze().cpu(), cmap=\"gray\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if show_difference_diagnostics:\n",
        "        print(f\"Equal? = {torch.all(true_img == adv_img).item()}\")\n",
        "        print(f\"Max Absolute Difference = {(true_img - adv_img).abs().max().item()}\")\n",
        "        print(f\"Mean Absolute Difference = {(true_img - adv_img).abs().mean().item()}\")\n",
        "\n",
        "sample_idx = torch.randint(len(adv_images), size=(1,)).item()\n",
        "display_adversarial_attack_example(\n",
        "    model,\n",
        "    true_img=X[sample_idx],\n",
        "    adv_img=adv_images[sample_idx],\n",
        "    true_label=y[sample_idx].item()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improved Vision Transformer Model vs. Baseline"
      ],
      "metadata": {
        "id": "cbYvspHPs20t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BASELINE MODEL:\n",
        "Experiment(  #0.3806\n",
        "  model_class=\"VisionTransformer\",\n",
        "  random_seed=[RANDOM_SEED],\n",
        "  experiment_store=experiment_store\n",
        ").df()"
      ],
      "metadata": {
        "id": "IUopFOXbv2FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPROVED MODEL:\n",
        "Experiment(  #0.9618\n",
        "    layer_norm=[\"none\"],\n",
        "    dropout=[0.2],\n",
        "    epochs_to_run=[20],\n",
        "    model_dim=[40],\n",
        "    num_layers=[8],\n",
        "    num_heads=[6],\n",
        "    head_dim=[32],\n",
        "    mlp_dim=[124],\n",
        "    QKV_bias=[False],\n",
        "    **shared_vision_transformers_experiment_args\n",
        ").df()"
      ],
      "metadata": {
        "id": "96D3yVO-sHB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yc12fzP3MvWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}